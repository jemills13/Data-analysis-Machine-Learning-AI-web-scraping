# -*- coding: utf-8 -*-
"""
Created on Thu Apr 12 08:47:42 2018

@author: Jonny Mills
"""
'''
Problem: there are too many applications for nursery school, and our school can't accommodate all of them. 
Our goal is to develop an ANN model that will evaluate applicants based on different attributes such as occupation of parents, 
family structure, financial standing, social conditions of the family, and health of the family.
Our goal is to use ANN to adopt a model that can ideally almost perfectly classify applicants based on their attribute information.
Our output variable will be a categorical variable that describes the level of recommendation for that applicant. 
There are three levels that we intend to classify applicants: acceptance, wait-list, and rejection.
Click link to view full research analysis.
https://docs.google.com/document/d/1xn2iAJ2RQph-gcFSKZHxA46EdpDoGiAUbx8P4UwB4rw/edit?usp=sharing
'''
##Here is a copy of the final code
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Import Libraries Section
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
print(__doc__)
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA as sklearnPCA
from sklearn.model_selection import cross_val_score
import seaborn as sns
import matplotlib.pyplot as plt
import datetime
from sklearn import svm



"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Load Data Section
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
#Dataset: https://archive.ics.uci.edu/ml/datasets/Nursery

data = pd.read_csv("C:/Users/Jonny Mills/Desktop/2nd semester Grad school/Artificial_Intelligence/Final project/nursery.csv",
                   delimiter=",", header=0,
                   names = ['parents', 'has_nurs', 'form', 'children', 'housing', 'finance', 'social','health','fake_taret_var','real_taret_var'])

print(data.shape)
data = data.dropna()
print (data.head())
data.dtypes

#http://pbpython.com/categorical-encoding.html
#obj_df = df.select_dtypes(include=['object']).copy()
#obj_df.head()


"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Pretreat Data Section
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

new_data = pd.get_dummies(data, columns = ['parents', 'has_nurs', 'form', 'children', 'housing', 'finance', 'social','health']) #creating dumy variables for all the x values.
#target var is already classified into 3 groups
print(new_data.shape)



x_data = new_data[list(new_data)[2::]] #exclude the target variable,
#colnames(x_data)
list(x_data) #examine column names. they look good
y_data = data['real_taret_var'] #real target var 
list(y_data)

x_data.head(n=5) #looks good
y_data.head(n=5) #looks good

x_data.shape
y_data.shape


np.random.seed(2016)
print (len(x_data))

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25, random_state=2017)

new_data.dtypes

"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Define Model Section (MLP Classifier)
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

hidden_layer_sizes = [50,100]
max_iter = [10,20]
learning_rate = [.05,.1]
solver = ['lbfgs','sgd','adam']

f = open('NurseryModelResultsMLPClassifier.csv','w')
f.write ('Hidden Layer Sizes, Max iterations, Learing Rate, Solver type, Train ERROR MSE, Test ERROR MSE \n') #needs to match the outstring line!

start_time = datetime.datetime.now()

test_list = []
for i in range(len(hidden_layer_sizes)):
    for j in range(len(max_iter)):
        for z in range(len(learning_rate)):
            for k in range(len(solver)):
                mlp = MLPClassifier(hidden_layer_sizes=(hidden_layer_sizes[i],), max_iter=max_iter[j], alpha=1e-4, #
                solver=solver[k], verbose=10, tol=1e-4, random_state=1, #tol tells it when to stop. 
                learning_rate_init=learning_rate[z]) #learning rate tells us how much we would move the weights at a time. 
                mlp.fit(x_train, y_train)
                
                ##########OUTPUT SECTION ##############
                print("fitting model now", hidden_layer_sizes[i], max_iter[j], learning_rate[z], solver[k])
                print("Training set score: %f" % mlp.score(x_train, y_train))
                cv = cross_val_score(mlp, x_test, y_test, cv=5)
                score = (sum(cv) / float(len(cv)))
                print("Test set score: %f" % score)
                test_list.append(mlp.score(x_test, y_test))
                
                outstring= str(hidden_layer_sizes[i]) +","+ str(max_iter[j]) + ","+ str(learning_rate[z]) +","+ str(solver[k]) + ","+ str(mlp.score(x_train, y_train))+","+ str(score) +"\n"
                f.write(outstring)
                
print(max(test_list))

stop_time = datetime.datetime.now()
print ("Time required for optimization:",stop_time - start_time)

f.close()



"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Support vector machine
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
kernal = []

clf = svm.SVC()
clf.fit(x_train, y_train) 
clf.predict(x_test)
clf.score(x_test, y_test)

#using the standard settings, we achieved a model with test accuracy of 0.986 accuracy
#http://scikit-learn.org/stable/modules/svm.html


"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Define Model Section (with PCA)
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
pca = sklearnPCA(n_components=5)
pca.fit(x_data)
x_data = pca.fit_transform(x_data)
x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_data, y_data, test_size=0.25, random_state=2016)

hidden_layer_sizes = [50,100]
max_iter = [10,20]
learning_rate = [.1,.2]
solver = ['lbfgs','sgd','adam']

f = open('NurseryModelResultsPCA.csv','w')
f.write ('Hidden Layer Sizes, Max iterations, Learing Rate, Solver type, Train ERROR MSE, Test ERROR MSE \n') #needs to match the outstring line!

start_time = datetime.datetime.now()

test_list = []
for i in range(len(hidden_layer_sizes)):
    for j in range(len(max_iter)):
        for z in range(len(learning_rate)):
            for k in range(len(solver)):
                mlp = MLPClassifier(hidden_layer_sizes=(hidden_layer_sizes[i],), max_iter=max_iter[j], alpha=1e-4, #
                solver=solver[k], verbose=10, tol=1e-4, random_state=1, #tol tells it when to stop. 
                learning_rate_init=learning_rate[z]) #learning rate tells us how much we would move the weights at a time. 
                mlp.fit(x_train_pca, y_train_pca)
                
                ##########OUTPUT SECTION ##############
                #cross_val_score(mlb, x_test, y_test, cv=cv)
                print("fitting model now", hidden_layer_sizes[i], max_iter[j], learning_rate[z], solver[k])
                print("Training set score: %f" % mlp.score(x_train_pca, y_train_pca))
                print("Test set score: %f" % mlp.score(x_test_pca, y_test_pca))
                test_list.append(mlp.score(x_test_pca, y_test_pca))
                
                outstring= str(hidden_layer_sizes[i]) +","+ str(max_iter[j]) + ","+ str(learning_rate[z]) +","+ str(solver[k]) + ","+ str(mlp.score(x_train_pca, y_train_pca))+","+ str(mlp.score(x_test_pca, y_test_pca))+"\n"
                f.write(outstring)
                
print(max(test_list))

stop_time = datetime.datetime.now()
print ("Time required for optimization:",stop_time - start_time)

f.close()
#Our MLPClassifier performs significantly better without PCA than with PCA





"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
Best model Section & confusion matrix
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
#plugging in the model parameters that yielded the lowest test error rate, we obtain our best model here 
best_mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=20, alpha=1e-4, #
                solver='adam', verbose=10, tol=1e-4, random_state=3142, #tol tells it when to stop. 
                learning_rate_init=.05) #learning rate tells us how much we would move the weights at a time. 
best_mlp.fit(x_train, y_train)
a = best_mlp.predict(x_test)
cm = confusion_matrix(y_test, a)
#add labels to confusion matrix


labels = ['Reject applicant', 'Wait List', 'Accpet applicant']




ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax, cmap="Blues",fmt='g'); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Reject', 'Wait-List', 'Accept']); ax.yaxis.set_ticklabels(['Reject', 'Wait-List', 'Accept']);
